# Настройка системы переводов

## Требования

1. **Ollama** - должен быть установлен и запущен локально
2. **Модель для перевода** - рекомендуется llama3.2 или аналогичная

## Установка Ollama

### Windows

```bash
# Скачайте и установите Ollama с официального сайта
# https://ollama.ai/download
```

### macOS

```bash
brew install ollama
```

### Linux

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

## Настройка модели

1. Запустите Ollama:

```bash
ollama serve
```

2. Скачайте модель (в отдельном терминале):

```bash
ollama pull llama3.2
```

3. Создайте файл `.env.local` в корне проекта:

```env
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

## Альтернативные модели

- `llama3.2:3b` - быстрее, но менее точный
- `llama3.2:1b` - очень быстрый, но менее точный
- `mistral` - хорошая альтернатива
- `codellama` - для технических текстов

## Использование

1. Запустите приложение: `npm run dev`
2. Откройте сайт в iframe
3. Выберите элемент для перевода
4. Введите язык (например: "русский", "английский", "español")
5. Нажмите "Элемент" для перевода

## Возможности

- ✅ Перевод отдельных элементов
- ✅ Сохранение HTML-разметки
- ✅ Поддержка 40+ языков
- ✅ Произвольный ввод языка
- ✅ Восстановление оригинального текста
- ✅ Контекстная информация для лучшего перевода

## Устранение неполадок

### Ollama не запускается

```bash
# Проверьте, что Ollama запущен
curl http://localhost:11434/api/tags
```

### Модель не найдена

```bash
# Скачайте модель заново
ollama pull llama3.2
```

### Медленный перевод

- Используйте меньшую модель: `llama3.2:3b`
- Увеличьте RAM для Ollama
- Закройте другие приложения
